{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load pickled data\n",
    "import pickle\n",
    "\n",
    "# TODO: Fill this in based on where you saved the training and testing data\n",
    "\n",
    "training_file = 'traffic-signs-data/train.p'\n",
    "testing_file = 'traffic-signs-data/test.p'\n",
    "\n",
    "with open(training_file, mode='rb') as f:\n",
    "    train = pickle.load(f)\n",
    "with open(testing_file, mode='rb') as f:\n",
    "    test = pickle.load(f)\n",
    "    \n",
    "X_train, y_train = train['features'], train['labels']\n",
    "X_test, y_test = test['features'], test['labels']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples = 39209\n",
      "Number of testing examples = 12630\n",
      "Image data shape = (32, 32, 3)\n",
      "Number of classes = 43\n"
     ]
    }
   ],
   "source": [
    "### Replace each question mark with the appropriate value.\n",
    "import numpy as np\n",
    "\n",
    "# TODO: Number of training examples\n",
    "n_train = train['features'].shape[0]\n",
    "\n",
    "# TODO: Number of testing examples.\n",
    "n_test = test['features'].shape[0]\n",
    "\n",
    "# TODO: What's the shape of an traffic sign image?\n",
    "image_shape = train['features'].shape[1:]\n",
    "\n",
    "# TODO: How many unique classes/labels there are in the dataset.\n",
    "n_classes = len(np.unique(train['labels']))\n",
    "\n",
    "print(\"Number of training examples =\", n_train)\n",
    "print(\"Number of testing examples =\", n_test)\n",
    "print(\"Image data shape =\", image_shape)\n",
    "print(\"Number of classes =\", n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data size: 31367, validation data size: 7842, test data size: 12630\n"
     ]
    }
   ],
   "source": [
    "### Generate data additional data (OPTIONAL!)\n",
    "### and split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "### Generate data additional data (OPTIONAL!)\n",
    "### and split the data into training/validation/testing sets here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_features, valid_features, train_labels, valid_labels = train_test_split(\n",
    "    train['features'],\n",
    "    train['labels'],\n",
    "    test_size=0.2,\n",
    "    random_state=832289)\n",
    "test_features = test['features']\n",
    "test_labels = test['labels']\n",
    "\n",
    "print (\"train data size: {0}, validation data size: {1}, test data size: {2}\".format(train_features.shape[0],\n",
    "                                                                                    valid_features.shape[0],\n",
    "                                                                                    test_features.shape[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten\n",
    "\n",
    "# Inputs\n",
    "features = tf.placeholder(tf.float32, (None, 32, 32, 3), name = \"features\")\n",
    "label = tf.placeholder(tf.int32, (None), name = \"label\")\n",
    "one_hot_label = tf.one_hot(label, n_classes)\n",
    "    \n",
    "# Layer 1: Convolutional\n",
    "weights1 = tf.Variable(tf.truncated_normal([5, 5, 3, 6], mean=0, stddev=0.1), name = 'weights')\n",
    "biases1 = tf.Variable(tf.zeros(6), name = 'biases')\n",
    "conv1 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(features, weights1, strides=[1, 1, 1, 1], padding='VALID'), biases1))\n",
    "conv1 = tf.nn.max_pool(conv1, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "# Layer 2: Convolutional\n",
    "weights2 = tf.Variable(tf.truncated_normal([5, 5, 6, 16], mean=0, stddev=0.1), name = 'weights')\n",
    "biases2 = tf.Variable(tf.zeros(16), name = 'biases')\n",
    "conv2 = tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(conv1, weights2, strides=[1, 1, 1, 1], padding='VALID'), biases2))\n",
    "conv2 = tf.nn.max_pool(conv2, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='VALID')\n",
    "\n",
    "#keep_prob = tf.placeholder(tf.float32, name = 'keep_prob')\n",
    "    \n",
    "# Layer 3: Full Connected\n",
    "flat_dim = 5 * 5 * 16\n",
    "fc0 = flatten(conv2)\n",
    "weights3 = tf.Variable(tf.truncated_normal([flat_dim, 120], mean=0, stddev=0.1), name = 'weights')\n",
    "biases3 = tf.Variable(tf.zeros(120), name = 'biases')\n",
    "fc1 = tf.nn.relu(tf.add(tf.matmul(fc0, weights3), biases3))\n",
    "#fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "# Layer 4: Full Connected\n",
    "weights4 = tf.Variable(tf.truncated_normal([120, 84], mean=0, stddev=0.1), name = 'weights')\n",
    "biases4 = tf.Variable(tf.zeros(84), name = 'biases')\n",
    "fc2 = tf.nn.relu(tf.add(tf.matmul(fc1, weights4), biases4))\n",
    "#fc2 = tf.nn.dropout(fc2, keep_prob)\n",
    "\n",
    "# Layer 5: Full Connected\n",
    "weights5 = tf.Variable(tf.truncated_normal([84, n_classes], mean=0, stddev=0.1), name = 'weights')\n",
    "biases5 = tf.Variable(tf.zeros(n_classes), name = 'biases')\n",
    "output = tf.add(tf.matmul(fc2, weights5), biases5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Train your model here.\n",
    "### Feel free to use as many code cells as needed.\n",
    "lrate = 0.001\n",
    "batch_size = 128\n",
    "epoches=5\n",
    "\n",
    "# define the cross entropy loss opertation\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(output, one_hot_label)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "# define the Adam optimizer and the training operation\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate = lrate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "\n",
    "# define the accuracy calculation operation\n",
    "prediction = tf.argmax(output, 1)\n",
    "correct_prediction = tf.equal(prediction, tf.argmax(one_hot_label, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# evaluate the accuracy given a set of images and labels\n",
    "def evaluate(x, y, step = 0):\n",
    "    num_examples = len(x)\n",
    "    total_accuracy = 0\n",
    "    total_loss = 0.0\n",
    "    sess = tf.get_default_session()\n",
    "    for offset in range(0, num_examples, batch_size):\n",
    "        end = offset + batch_size\n",
    "        batch_x, batch_y = x[offset : end], y[offset : end]\n",
    "        accuracy, loss = sess.run([accuracy_operation, loss_operation] ,\n",
    "                                     feed_dict={features: batch_x, label: batch_y})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "        total_loss += (loss * len(batch_x))\n",
    "    return total_accuracy / num_examples, total_loss / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Average Loss = 3.507\n",
      "Train Accuracy = 0.055\n",
      "Valid Accuracy = 0.051\n",
      "\n",
      "EPOCH 2 ...\n",
      "Average Loss = 3.487\n",
      "Train Accuracy = 0.058\n",
      "Valid Accuracy = 0.055\n",
      "\n",
      "EPOCH 3 ...\n",
      "Average Loss = 3.487\n",
      "Train Accuracy = 0.057\n",
      "Valid Accuracy = 0.056\n",
      "\n",
      "EPOCH 4 ...\n",
      "Average Loss = 3.486\n",
      "Train Accuracy = 0.057\n",
      "Valid Accuracy = 0.055\n",
      "\n",
      "EPOCH 5 ...\n",
      "Average Loss = 3.486\n",
      "Train Accuracy = 0.057\n",
      "Valid Accuracy = 0.055\n",
      "\n",
      "Test accuracy = 0.057\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    num_examples = len(train_features)\n",
    "    print(\"Training...\")\n",
    "    print()\n",
    "    step = 0\n",
    "    for i in range(epoches):\n",
    "        train_featues, train_labels = shuffle(train_features, train_labels)\n",
    "        for offset in range(0, num_examples, batch_size):\n",
    "            end = offset + batch_size\n",
    "            batch_features, batch_labels = train_features[offset : end], train_labels[offset : end]\n",
    "            sess.run(training_operation, feed_dict={features: batch_features, label: batch_labels})\n",
    "        train_accuracy, loss = evaluate(train_features, train_labels, step)\n",
    "        valid_accuracy, _ = evaluate(valid_features, valid_labels, step)\n",
    "        step += 1\n",
    "        print(\"EPOCH {} ...\".format(i+1))\n",
    "        print(\"Average Loss = {:.3f}\".format(loss))\n",
    "        print(\"Train Accuracy = {:.3f}\".format(train_accuracy))\n",
    "        print(\"Valid Accuracy = {:.3f}\".format(valid_accuracy))\n",
    "        print()\n",
    "        \n",
    "    # Evaluate on the test datatf.truncated_normal        \n",
    "    test_accuracy, loss = evaluate(test_features, test_labels)\n",
    "    print(\"Test accuracy = {:.3f}\".format(test_accuracy))\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
